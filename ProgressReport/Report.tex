% \title{\Large \textbf{Modelling the Age of Abalone}}
% \author{\small 450132759}
% \date{\small October 29, 2018}\textsl{}



\documentclass[12pt,twocolumn]{article}
%\usepackage{fullpage} % full parge margins
\usepackage[margin=1in]{geometry}
\usepackage{hyperref} % allows for linking urls
\usepackage[superscript,biblabel]{cite} % superscript while citing
\usepackage{booktabs}
\usepackage{graphicx} % allows for adding figures
\usepackage{subcaption} % allows for adding subfigures
\usepackage{enumitem} % allows for lists
\usepackage{amsmath} % allows for math equations
\usepackage{gensymb} % degree symbol, usage: \degree
\usepackage{csvsimple}
\usepackage{booktabs} % to generate booktabs style tables
\usepackage{lscape}
\usepackage{siunitx}
\usepackage{tikz} % To generate the plot from csv
\usepackage{pgfplots}
\pgfplotsset{compat=newest} % Allows to place the legend below plot
\usepackage{placeins} % Use in conjunction with \FloatBarrier to limit floating of figures
\usepackage{sectsty} % allows for defining section styles
\sectionfont{\fontsize{14}{13}\selectfont} % defines font size of sections
\subsectionfont{\fontsize{12}{13}\selectfont} % defines font size of sections

\begin{document}
	\begin{center}
		\textbf{\Large Thesis A Progress Report} \\\vspace{4mm}
		\textbf{\Large Electrical Load Modelling With Predictive Machine Learning} \\\vspace{4mm}
		\texttt{PHUA, Benjamin\\451032759}
	\end{center}

	\section{Introduction}
		* Why is this topic important to the reader?
		* What is the problem?
		* How do we plan on solving this problem?
		* What is the past work done, and why is what we're doing different/significant?


		A home-based intelligent energy conservation system needs to know what appliances (or loads) are being used in the home and when they are being used in order to provide intelligent feedback or to make intelligent decisions. This analysis task is known as load disaggregation or non-intrusive load monitoring (NILM). The datasets used for NILM research generally contain real power readings, with the data often being too coarse for more sophisticated analysis algorithms, and often covering too short a time period.

		Currently, much of the world is focused on reducing electricity consumption; our increase in consumption is neither economically nor environmentally sustainable. Additionally, there is a growing consensus that environmental and economical sustainability are inextricably linked. As the cost of power rises, we must find technological solutions that help reduce and optimize energy use. For homeowners and occupants, one way to achieve this goal is to monitor their power consumption by understanding appliance usage through an effective eco-feedback device or display mechanism.

		When designing and implementing an intelligent energy conservation system for the home, it is essential to have insight into the activities and actions of the occupants. In particular, it is important to understand what appliances are being used and when. In the computational sustainability research community this is known as load disaggregation or nonintrusive load monitoring (NILM) (Section II). 


	\section{Literature Review}
		\subsection{Non-intrusive Load Monitoring}

			\subsubsection*{Background}
			 \quad Non-intrusive load monitoring (NILM) is a useful technique which is used to determine the energy consumption of residential and commercial appliances by analysing the aggregate load measured by the main power meter in a building. By analysing changes in voltage and current read through the meter, a model can be made to determine which appliances were consuming power as well as their level of comsumption. This is often used by utilities in smart meters to survey the power usage in residential and commercial buildings. The benefits of using this method is not requiring the installation of expensive metering for each individual appliance, and as a result, could be placed at many sites at a low cost. 

			\subsubsection*{Underlying Principle}
			\quad Based on the analysis of the aggregated data measured from a single meter ouside the building, the energy consumption of individial appliances can be determinted. The basic principle behind NILM is in recognising a change in voltage and current drawn by an appliance. For example, if a 2.3kW kettle was switched on followed subsequently by a 200W desktop computer, and the kettle was then switched off, the power consumption levels can be analysed to discriminate between the appliances by looking at the on and off signals and matching them with the same appliance. In this case, if the meter measures a reduction of 2.3kW consumed, it would match the off signal to the kettle instead of to the computer. Each appliance's power signature is sufficiently different such that there is an appropriate discrimination between them, which can be deduced through analysis. By also measuring the real and reactive power, two appliances with the same power drawn can also be discriminated by differences in their complex impedance. In addition to analysing the active and reactive power, techniques based on current waveform characteristics, harmonic frequencies, steady-state behavior, and fundamental frequencies exists that also aid in load identification. 

			\subsubsection*{Feature Analysis}
			\quad To create a good model for the aggregate data, it is critical to understand the underlying behavior of the individual appliances for a more complete analysis. Domestic appliances generally fall into these categories: two-state appliances, multi-state appliances, continuously-varying power appliances, and permanent consumer appliances (Zoha et al., 2012), (Zeifman, and Roth, 2011), (Baranski, and Voss, 2003), (Norford, and Leeb, 1996). Our algorithm must, to the best of its ability, identify these loads. \newline

			Two-state appliances are those which are either ON or OFF at any given time. Light bulbs, fans, water kettles, and toasters are examples of such appliances. When in addition there is a STANDBY state, these are known as multi-state appliances. Examples of such are washing machines and dishwashers. Permanent consumer appliances are those which are in a permanent ON state, and examples of these are emergency exit signs and smoke detectors. Continuously-varying power appliances are those that do not consume constant power. These are anticipated to pose the most difficulties when creating a model as they do not have well-defined behavior. This is a limitation of NILM that must be taken into account when running the analysis. By comparing known behavior of typical appliances in the data, we could then start to create a model that would identify appliances using algorithms. The real challenge here is to detect complex, changing loads in our algorithm. 
			
		\subsection{AMPds}

			\subsubsection*{Background}
				\quad When performing load disaggregation on a residential building, it is essential to have insight into the behavior and activities of all appliances, especially when and which appliances are being used. A popular dataset that is used in current research into NILM is the Almanac of Minutely Power dataset (AMPds), which contains one year of data that includes 11 measurements at one minute intervals for 21 sub-meters. This was recorded from April 1, 2012 to March 31, 2013, on a house in Vancouver. Additionally, natural gas and water metering is also included in the dataset. The dataset contains 524,544 valid readings per sub-meter. The variables stored in the dataset are as follows:

				\begin{figure}[!htbp]
					\centering
					\includegraphics[width=0.7\linewidth]{source/data_desc}
					\caption{Power Measurements Captured}
					\label{fig:varnorm}
				\end{figure}

				\begin{figure}[!h]
					\centering
					\includegraphics[width=0.5\linewidth]{source/appliance_id}
					\caption{Appliance IDs}
					\label{fig:varnorm}
				\end{figure}

				\quad Furthermore, a new dataset called AMPds Version 2 has been recorded that contains 1,051,200 readings per meter for 2 years of monitoring, along with weather data. This additional dataset would be useful in gaining more insight in our model. 

		\subsection{Short Term Load Forecasting (STLF)}

			\quad Forecasting electricity loads enables utility providers to model and forecast power loads to maintain a balance between production and demand, and has application in electricity pricing, scheduling, capacity planning, and Demand Response Management (DRM). The DRM controls electricity consumption at the consumer side and improves the efficiency and reduces cost in the grid. The primary differentiator between classification of forecasting models is in how far forward the model predicts. They fall generally short-term forecast ranging between on ehour and one week, medium term ranging between one week and one year, and long-term spanning a time of more than one year. Short term load forecasting (STLF) is essential in the everyday operation of electricity generation and consumption, and with the popularity of alternative energy, STLF is gaining importance among industries, residential complexes, and corporate buildings. \newline

			\quad Early methods used to tackle STLF include multiple linear regression, exponential smoothing, and time series algorithms such as ARIMA. However, due to the large number of behavioral variations and inherent non-linearity, these methods performed poorly and instead gave way to computational intelligence methods. These include clustering methods, support vector machine (SVM), and artificial neural networks, which are nonlinear and are more suited for STLF. The literature reviews show that these methods greatly improves accuracy, and a few of these algorithms will be discussed next.

		\subsection{Forecasting Algorithms}

			\subsubsection*{Neural Networks}
				\quad Neural networks are used to fit highly non-linear data and the idea was inspired by the way the brain takes in multiple information from neurons to produce an outcome. A typical neural network consists of an input layer, a hidden layer, and an output layer, each consisting of nodes or neurons. 

				\begin{figure}[!htbp]
					\centering
					\includegraphics[width=\linewidth]{source/neural}
					\caption{Representation of a neural network}
				\end{figure}

				The outputs of succeeding layers are calculated by forward propagation while weights of the neurons are determined by training the neurons with backpropagation. \newline

				The nodes in the hidden layer are propagated through a sum of the inputs, added to a bias, and then passed through an activation function. A commonly used activation function is the sigmoid function, which only outputs numbers in the range (0,1). \newline

				\quad Before we train our network, we first need a way to quantify how "good" it is doing so it can try to do "better". Which leads to the definition of loss in our neural network. One way loss can be calculated is by using the mean squared error (MSE), which takes the difference between the actual output and the expected output, squares this value, and sums them up. Other ways this can be calculated include looking at the Mean Absolute Percentage error (MAPE), Mean Absolute Error (MAE), and Root Mean Squared Error (RMSE). With the loss function, we can take partial derivatives to train our network to reduce the loss through each iteration. This is known as backpropagation and is done by updating the weights at each of the nodes. An optimisation algorithm called stochastic gradient descent (SGD) tells us how to change our weights and biases to minimise loss. \newline

			\subsubsection*{Long Short Term Memory (LSTM)}
				\quad One shortcoming of traditional neural networks is that it is difficult to predict future events using data from past events - they accept a fixed-sized vector as input and produce a fixed-sized vector as outputm and they perform this mapping using a finite amount of computational steps. This is addressed by recurrent neural networks, which takes multiple copies of the same network, and passes on the message to its successor. The chain-like structure of recurrent neural networks show an inherent similarity to sequences and lists, and has seen incredible success in the applications of speech recognition, language modeling, translation, and so on. 

				\begin{figure}[!htbp]
					\centering
					\includegraphics[width=\linewidth]{source/rnn}
					\caption{Representation of RNN}
				\end{figure}

				\quad Recurrent neural networks however, do suffer from short-term memory. If a sequence is long enough, they find it difficult to carry information stored in early steps to later stages. In our dataset for example, we want our short term forecasting to take into account data captured perhaps a few months earlier, instead of the just the data captured a few days before. This is due to the vanishing gradient problem. Gradient values that are used to update the weights shrinks as it back propagates through time. This means that after a few propagations, it doesn't contribute much to learning anymore. \newline

				\quad This is where Long Short Term Memory (LSTM) comes to to rescue. They have internal machanisms called gates that regulate the flow of information. These gates learn which data in a sequence is important to keep. They pick up important signal that pass through the network and feeds this to later nodes. Information passes through a Forget Gate, which determines whether the information should be kept or forgotten. 

				\begin{figure}[!htbp]
					\centering
					\includegraphics[width=\linewidth]{source/lstm}
					\caption{Representation of LSTM}
				\end{figure}

				LSTM-based method is capable of forecasting accurately the complex electric load time series with a long forecasting horizon by exploiting the long-term dependencies. Past papers have shown that the average MAPE for LSTM networks outperform other methods. 

	\section{Progress Report}
		\textbf{Weeks 1-6}

			Researched past papers on Non-Intrusive Load Modelling (NILM), Short Term Load Forecasting, and on the AMPds dataset. I have used the following research articles from IEEE to further my understanding of the topic.

			\begin{itemize}
				\item Short-Term Residential Load Forecasting Based on Resident Behaviour Learning, Weicong Kong
				\item A Deep Neural Network Model for Short-Term Load Forecast Based on Long Short-Term Memory Network and Convolutional Neural Network, Chujie Tian
				\item Short-Term Electrical Load Forecasting using Predictive Machine Learning Models, Karun Warrior
				\item A Hierarchical Hidden Markov Model Framework for Home Appliance Modeling, Weicong Kong
				\item Optimal Deep Learning LSTM Model for Electric Load Forecasting using Feature Selection and Genetic Algorithm: Comparison with Machine Learning Approaches, Salah Bouktif
			\end{itemize}

		\textbf{Weeks 5-6}
			
			Reviewed core machine learning topics such as neural networks, decision trees, clustering. Researched algorithms used in NILM and STLF such as LSTM neural networks. Took an intro to machine learning in Python tutorial on Datacamp. 
			\newline

		\textbf{Weeks 7-10}

			Worked on a TensorFlow tutorial on DataCamp to train neural networks. Tensorflow is an open-source software library used for machine learning applications such as neural networks. 
			\newline

			\begin{figure}[!h]
					\centering
					\includegraphics[width=\linewidth]{source/tensorflow}
					\caption{DataCamp TensorFlow Tutorial}
			\end{figure}

			An outline of the core topics learned are as follows:
			\begin{itemize}
				\item Introduction to TensorFlow and graph-based computation
				\item Linear Regression in TensorFlow - I learned how to construct loss functions and minimize them to find optimal parameter values for linear models.
				\item Neural Networks in TensorFlow - I learned how to define dense layers and apply activation functions.
				\item High Level APIs in TensorFlow - I learned how to train and validate my data with Keras.
			\end{itemize}
		\textbf{Weeks 12-13}

			Worked on another DataCamp tutorial on Building Models with Keras.

			Similar to TensorFlow, Keras is used to build complex neural networks. The project will be based on Keras as it allows for easy integration of LSTM and Fast-Forward Neural Network (FFNN) models.
			\newline

			\begin{figure}[!h]
				\centering
				\includegraphics[width=\linewidth]{source/keras}
				\caption{DataCamp Keras Tutorial}
			\end{figure}

			An outline of the core topics learned are as follows:
			\begin{itemize}
				\item Forward Propagation and Multi-layer neural networks
				\item Calculating model errors, gradient descent, updating weights, and backpropagation
				\item Fitting the model and making predictions
			\end{itemize}

	\section{Revised Proposal/Plan}
		\textbf{Winter Break}
		\begin{itemize}
			\item Create preliminary model
			\item Train neural network using a subset of the data
			\item Compare different algorithms to find the best fit
			\item Optimise model and performing testing
		\end{itemize}
		\textbf{Semester 2 - Weeks 1-4}
		\begin{itemize}
			\item Finalise model and compile results
			\item Analyse findings and compare to current research
			\item Start a draft of final thesis
		\end{itemize}
		\textbf{Semester 2 - Weeks 5-8}
		\begin{itemize}
			\item Finish writing core component of thesis
			\item Summarise findings and conclude
			\item Perform further testing if time permits
		\end{itemize}
		\textbf{Semester 2 - Weeks 9-12}
		\begin{itemize}
			\item Prepare for final presentation
		\end{itemize}

	\begin{thebibliography}{99}
		\bibitem{Kong18} Kong, W. (2019). Short-Term Residential Load Forecasting Based on Resident Behaviour Learning - IEEE Journals \& Magazine. [online] Ieeexplore.ieee.org. Available at: \url{https://ieeexplore.ieee.org/document/7887751/}.

		\bibitem{Tian} Tian, C., Ma, J., Zhang, C. and Zhan, P. (2018). A Deep Neural Network Model for Short-Term Load Forecast Based on Long Short-Term Memory Network and Convolutional Neural Network. Energies, 11(12), p.3493.

		\bibitem{Warrior} Warrior, K., (2018). Short-Term Electrical Load Forecasting using Predictive Machine Learning Models

		\bibitem{Kong19} Kong, W. (2019). A Hierarchical Hidden Markov Model Framework for Home Appliance Modeling - IEEE Journals \& Magazine. [online] Ieeexplore.ieee.org. Available at: \url{https://ieeexplore.ieee.org/document/7738573/}.

		\bibitem{Bouktif} Bouktif, S., Fiaz, Ouni and Serhani (2019). Optimal Deep Learning LSTM Model for Electric Load Forecasting using Feature Selection and Genetic Algorithm: Comparison with Machine Learning Approaches †. [online] Ideas.repec.org. Available at: \url{https://ideas.repec.org/a/gam/jeners/v11y2018i7p1636-d153976.html}.



	
	\end{thebibliography}
\end{document}





























