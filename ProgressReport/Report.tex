% \title{\Large \textbf{Modelling the Age of Abalone}}
% \author{\small 450132759}
% \date{\small October 29, 2018}\textsl{}



\documentclass[12pt,twocolumn]{article}
\usepackage{fullpage} % full parge margins
% \usepackage[margin=1in]{geometry}
\usepackage{hyperref} % allows for linking urls
\usepackage[superscript,biblabel]{cite} % superscript while citing
\usepackage{booktabs}
\usepackage{graphicx} % allows for adding figures
\usepackage{subcaption} % allows for adding subfigures
\usepackage{enumitem} % allows for lists
\usepackage{amsmath} % allows for math equations
\usepackage{gensymb} % degree symbol, usage: \degree
\usepackage{csvsimple}
\usepackage{booktabs} % to generate booktabs style tables
\usepackage{lscape}
\usepackage{siunitx}
\usepackage{tikz} % To generate the plot from csv
\usepackage{pgfplots}
\pgfplotsset{compat=newest} % Allows to place the legend below plot
\usepackage{placeins} % Use in conjunction with \FloatBarrier to limit floating of figures
\usepackage{sectsty} % allows for defining section styles
\sectionfont{\fontsize{14}{13}\selectfont} % defines font size of sections
\subsectionfont{\fontsize{12}{13}\selectfont} % defines font size of sections

\begin{document}
	\begin{center}
		\textbf{\Large Thesis Progress Report} \\\vspace{4mm}
		\textbf{\Large Electrical Load Modelling With Predictive Machine Learning} \\\vspace{4mm}
		\texttt{PHUA, Benjamin\\451032759}
	\end{center}

	\section{Introduction}

		Recent estimates indicate that in the United States alone, products containing power supplies consume over 200 billion kWh of energy each year. Furthermore, worldwide sales of power supplies are expected to grow by 15\% each year. To significantly reduce peak energy consumption, it is apparent that one way to bring about a significant reduction in energy demand is to introduce autonomous demand-side energy management features. For example, low priority appliances could detect the operation of a large energy consumer such as a hot water pump, and could place themselves in a low power mode. 

		With the rolling out of advanced metering infrastructure around residential networks, being able to understand their usage patterns and monitoring their power consumption would be extremely beneficial to homeowners and occupants, and could help them reduce and optimise energy use. When designing and implementing an intelligent energy conservation system for the home, having insight into the appliance consumption is a must, especially which are being used and when. This analysis is known as load disaggregation or non-intrusive load monitoring (NILM). 

		Although we are observing a large number of data being mined from smart meters, developing and selecting accurate models to analyse the data is a challenging task. Due to the non-linear nature of the data, deep learning approaches have shown to provide better performance to predict electrical load in real world cases, and one in particular - Long Short Term Memory (LSTM) stands out. This paper summarises the key concepts of NILM and then introduces LSTM as a candidate in furthering the research into load disaggregation.

	\section{Literature Review}
		\subsection{Non-intrusive Load Monitoring}

			\subsubsection*{Background}
			 \quad We are observing large scale deployment of smart meters in the residential environments, traditional smart meters are only able to measure energy consumption data at a house level granularity. In order to implement a precise demand-response functionality, a much finer granularity of information is required. To achieve this, research efforts have led to the development of Appliance Load Monitoring (ALM) methods. The goal of ALM is to perform detailed energy sensing and to provide information on the breakdown of the energy spent. This would further enable the automated energy management systems to profile high energy consuming appliances, allowing them to devise energy conservation strategies such as re-scheduling of high power demanding operations for the off-peak times.

			 The two major approches to ALM are Intrusive Load Monitoring (ILM) and Non-Intrusive Load Monitoring (NILM). They are alternatively referred to as distributed sensing and single point sensing, respectively, as ILM requires one or nore than one sensor per appliance, where NILM just requires a single point of monitoring for the entire building.  Although the ILM method is more accurate in measuring appliance specific energy consumption, practical disadvantages include high costs for multiple sensor installations. Consequently, academic researchers have focused their attention to NILM based approaches. 

			\subsubsection*{Underlying Principle}
			\quad Based on the analysis of the aggregated data measured from a single meter outside the building, the energy consumption of individual appliances can be determined. The basic principle behind NILM is in recognising a change in voltage and current drawn by an appliance. For example, if a 2.3kW kettle was switched on followed subsequently by a 200W desktop computer, and the kettle was then switched off, the power consumption levels can be analysed to discriminate between the appliances by looking at the on and off signals and matching them with the same appliance. In this case, if the meter measures a reduction of 2.3kW consumed, it would match the off signal to the kettle instead of to the computer. Each appliance's power signature is sufficiently different such that there is an appropriate discrimination between them, which can be deduced through analysis. By also measuring the real and reactive power, two appliances with the same power drawn can also be discriminated by differences in their complex impedance. In addition to analysing the active and reactive power, techniques based on current waveform characteristics, harmonic frequencies, steady-state behaviour, and fundamental frequencies exists that also aid in load identification. 

			\subsubsection*{Feature Analysis}
			\quad To create a good model for the aggregate data, it is critical to understand the underlying behaviour of the individual appliances for a more complete analysis. Domestic appliances generally fall into these categories: two-state appliances, multi-state appliances, continuously-varying power appliances, and permanent consumer appliances (Zoha et al., 2012), (Zeifman, and Roth, 2011), (Baranski, and Voss, 2003), (Norford, and Leeb, 1996). Our algorithm must, to the best of its ability, identify these loads. \newline

			Two-state appliances are those which are either ON or OFF at any given time. Light bulbs, fans, water kettles, and toasters are examples of such appliances. When in addition there is a STANDBY state, these are known as multi-state appliances. Examples of such are washing machines and dishwashers. Permanent consumer appliances are those which are in a permanent ON state, and examples of these are emergency exit signs and smoke detectors. Continuously-varying power appliances are those that do not consume constant power. These are anticipated to pose the most difficulties when creating a model as they do not have well-defined behaviour. This is a limitation of NILM that must be taken into account when running the analysis. By comparing known behaviour of typical appliances in the data, we could then start to create a model that would identify appliances using algorithms. The real challenge here is to detect complex, changing loads in our algorithm. 

			\begin{figure}[!h]
					\centering
					\includegraphics[width=\linewidth]{source/loadprofiles}
					\caption{Category of Consumer Appliances}
				\end{figure}

			\subsubsection*{Past Research}
			\quad Many of the research performed have been in either high frequency sampling or low frequency sampling. High-frequency sampling (1kHz-20kHz) captures more readily transient signals, and in their paper titled ``Transient event detection for nonintrusive load monitoring and demand side management using voltage distortion'', S. Leeb and L. Norford introduced the idea of using harmonic characteristics of power signal to analyse the harmonic of transicent signals. They were able to identify the transient profiles of different classes of loads by identifying load operation using voltage distortion caused by transient load currents. They did this by using the fast Fourier transform to obtain the harmonic coefficients, which constituted the feature vector of the equipment for load identification. Further works that have been carried out include load identification based on integer programming method by K. Suzuki and S. Inagaki. S.R. Shaw detected the envelop signal based on the idea of least square, in which the load is identified by the residuals. All of these papers were based on high frequency sampling, but the main drawback of this method however, is that the high-frequency sampling meters are often custom-built and expensive due to sophisticated hardware and are tailored to the type of features that needs to be extracted from the signal. This method is also not entirely non-intrusive, and separate devices have to in installed for training, visualisation, and communication to the grid. Privacy concerns have also been raised as user activity can be easily detected, interpreted, and monitored. \newline

			Low-frequency sampling creates a larger challenge for the disaggregation process, but the one major advantage it has over high-frequency sampling methods is that they are easily available from smart meters all over the world. The major issue with the low sampling rate is that low energy consuming devices have difficulty being detected as the switching events are not very prominent. However, high energy consuming appliances such as water heaters and washing machines can still be identified with reasonable precision. K. Basu in ``Time series distance-based methods for non-intrusive load monitoring in residential buildings'' found that the states of various residential appliances can be identified even with 10-minute interval readings. Stephen Makonin proposed identification algorithm based on Hidden Markov chain, and demonstrated this through the design of a super-state HMM and sparse Viterbi algorithm. The merit of this lies in its ability to run in real-time and can disaggregate a model with a large number of super-states. This would allow occupants to defer high-consuming loads to when the per kWh is low, and see savings on their power utility bill. N. Henao and K. Agbossou proposed a technique using subtractive clustering and the maximum likelihood classifier. The Genetic K-means and agglomerative clustering approaches have been investigated to automatically determine the total number of appliance clusters from the load data. Each cluster is assumed to be a linear combination of multiple appliance sources, which are further broken down into individual sources. However several drawbacks and challenges are highlighting by the author himself. Most of the false negative events were due to low consumption appliances such as light bulbs due to the similarities in the level of consumption. In the case of multi-state appliances, source reconstruction becomes challenging as clusters were formed due to multiple states, which resulted in a mixing of events. Even in the face of such challenges, low-frequency sampling methods is apparent in its merits and continue to experience the most amount of academic research to date. 
			
		\subsection{AMPds}

			\subsubsection*{Background}
				\quad When performing load disaggregation on a residential building, it is essential to have insight into the behaviour and activities of all appliances, especially when and which appliances are being used. A popular dataset that is used in current research into NILM is the Almanac of Minutely Power dataset (AMPds), which contains one year of data that includes 11 measurements at one minute intervals for 21 sub-meters. This was recorded from April 1, 2012 to March 31, 2013, on a house in Vancouver. Additionally, natural gas and water metering is also included in the dataset. The dataset contains 524,544 valid readings per sub-meter. The variables stored in the dataset are as follows:

				\begin{figure}[!htbp]
					\centering
					\includegraphics[width=0.7\linewidth]{source/data_desc}
					\caption{Power Measurements Captured}
					\label{fig:varnorm}
				\end{figure}

				\begin{figure}[!h]
					\centering
					\includegraphics[width=0.5\linewidth]{source/appliance_id}
					\caption{Appliance IDs}
					\label{fig:varnorm}
				\end{figure}

				\quad Furthermore, a new dataset called AMPds Version 2 has been recorded that contains 1,051,200 readings per meter for 2 years of monitoring, along with weather data. This additional dataset would be useful in gaining more insight in our model. 

		\subsection{Short Term Load Forecasting (STLF)}

			\quad Forecasting electricity loads enables utility providers to model and forecast power loads to maintain a balance between production and demand, and has application in electricity pricing, scheduling, capacity planning, and Demand Response Management (DRM). The DRM controls electricity consumption at the consumer side and improves the efficiency and reduces cost in the grid. The primary differentiator between classification of forecasting models is in how far forward the model predicts. They fall generally short-term forecast ranging between one hour and one week, medium term ranging between one week and one year, and long-term spanning a time of more than one year. Short term load forecasting (STLF) is essential in the everyday operation of electricity generation and consumption, and with the popularity of alternative energy, STLF is gaining importance among industries, residential complexes, and corporate buildings. \newline

			\quad Early methods used to tackle STLF include multiple linear regression, exponential smoothing, and time series algorithms such as ARIMA. However, due to the large number of behavioural variations and inherent non-linearity, these methods performed poorly and instead gave way to computational intelligence methods. These include clustering methods, support vector machine (SVM), and artificial neural networks, which are nonlinear and are more suited for STLF. The literature reviews show that these methods greatly improves accuracy, and a few of these algorithms will be discussed next.

		\subsection{Forecasting Algorithms}

			\subsubsection*{Neural Networks}
				\quad Neural networks are used to fit highly non-linear data and the idea was inspired by the way the brain takes in multiple information from neurons to produce an outcome. A typical neural network consists of an input layer, a hidden layer, and an output layer, each consisting of nodes or neurons. 

				\begin{figure}[!htbp]
					\centering
					\includegraphics[width=\linewidth]{source/neural}
					\caption{Representation of a neural network}
				\end{figure}

				The outputs of succeeding layers are calculated by forward propagation while weights of the neurons are determined by training the neurons with backpropagation. \newline

				The nodes in the hidden layer are propagated through a sum of the inputs, added to a bias, and then passed through an activation function. A commonly used activation function is the sigmoid function, which only outputs numbers in the range (0,1). \newline

				\quad Before we train our network, we first need a way to quantify how "good" it is doing so it can try to do "better". Which leads to the definition of loss in our neural network. One way loss can be calculated is by using the mean squared error (MSE), which takes the difference between the actual output and the expected output, squares this value, and sums them up. Other ways this can be calculated include looking at the Mean Absolute Percentage error (MAPE), Mean Absolute Error (MAE), and Root Mean Squared Error (RMSE). With the loss function, we can take partial derivatives to train our network to reduce the loss through each iteration. This is known as backpropagation and is done by updating the weights at each of the nodes. An optimisation algorithm called stochastic gradient descent (SGD) tells us how to change our weights and biases to minimise loss. \newline

			\subsubsection*{Long Short Term Memory (LSTM)}
				\quad One shortcoming of traditional neural networks is that it is difficult to predict future events using data from past events - they accept a fixed-sized vector as input and produce a fixed-sized vector as output and they perform this mapping using a finite amount of computational steps. This is addressed by recurrent neural networks, which takes multiple copies of the same network, and passes on the message to its successor. The chain-like structure of recurrent neural networks show an inherent similarity to sequences and lists, and has seen incredible success in the applications of speech recognition, language modelling, translation, and so on. 

				\begin{figure}[!htbp]
					\centering
					\includegraphics[width=\linewidth]{source/rnn}
					\caption{Representation of RNN}
				\end{figure}

				\quad Recurrent neural networks however, do suffer from short-term memory. If a sequence is long enough, they find it difficult to carry information stored in early steps to later stages. In our dataset for example, we want our short term forecasting to take into account data captured perhaps a few months earlier, instead of the just the data captured a few days before. This is due to the vanishing gradient problem. Gradient values that are used to update the weights shrinks as it back propagates through time. This means that after a few propagations, it doesn't contribute much to learning anymore. \newline

				\quad This is where Long Short Term Memory (LSTM) comes to play. They have internal mechanisms called gates that regulate the flow of information. These gates learn which data in a sequence is important to keep. They pick up important signal that pass through the network and feeds this to later nodes. Information passes through a Forget Gate, which determines whether the information should be kept or forgotten. 

				\begin{figure}[!htbp]
					\centering
					\includegraphics[width=\linewidth]{source/lstm}
					\caption{Representation of LSTM}
				\end{figure}

				LSTM-based method is capable of forecasting accurately the complex electric load time series with a long forecasting horizon by exploiting the long-term dependencies. Past papers have shown that the average MAPE for LSTM networks outperform other methods. 

	\section{Progress Report}
		\textbf{Weeks 1-6}

			Researched past papers on Non-Intrusive Load Monitoring (NILM), Short Term Load Forecasting, and on the AMPds dataset. I have used the following research articles from IEEE to further my understanding of the topic.

			\begin{itemize}
				\item Short-Term Residential Load Forecasting Based on Resident Behaviour Learning, Weicong Kong
				\item A Deep Neural Network Model for Short-Term Load Forecast Based on Long Short-Term Memory Network and Convolutional Neural Network, Chujie Tian
				\item Short-Term Electrical Load Forecasting using Predictive Machine Learning Models, Karun Warrior
				\item A Hierarchical Hidden Markov Model Framework for Home Appliance Modeling, Weicong Kong
				\item Optimal Deep Learning LSTM Model for Electric Load Forecasting using Feature Selection and Genetic Algorithm: Comparison with Machine Learning Approaches, Salah Bouktif
			\end{itemize}

		\textbf{Weeks 5-6}
			
			Reviewed core machine learning topics such as neural networks, decision trees, clustering. Researched algorithms used in NILM and STLF such as LSTM neural networks. Took an intro to machine learning in Python tutorial on Datacamp. 
			\newline

		\textbf{Weeks 7-10}

			Worked on a TensorFlow tutorial on DataCamp to train neural networks. Tensorflow is an open-source software library used for machine learning applications such as neural networks. 
			\newline

			\begin{figure}[!h]
					\centering
					\includegraphics[width=\linewidth]{source/tensorflow}
					\caption{DataCamp TensorFlow Tutorial}
			\end{figure}

			An outline of the core topics learned are as follows:
			\begin{itemize}
				\item Introduction to TensorFlow and graph-based computation
				\item Linear Regression in TensorFlow - I learned how to construct loss functions and minimize them to find optimal parameter values for linear models.
				\item Neural Networks in TensorFlow - I learned how to define dense layers and apply activation functions.
				\item High Level APIs in TensorFlow - I learned how to train and validate my data with Keras.
			\end{itemize}
		\textbf{Weeks 12-13}

			Worked on another DataCamp tutorial on Building Models with Keras.

			Similar to TensorFlow, Keras is used to build complex neural networks. The project will be based on Keras as it allows for easy integration of LSTM and Fast-Forward Neural Network (FFNN) models.
			\newline

			\begin{figure}[!h]
				\centering
				\includegraphics[width=\linewidth]{source/keras}
				\caption{DataCamp Keras Tutorial}
			\end{figure}

			An outline of the core topics learned are as follows:
			\begin{itemize}
				\item Forward Propagation and Multi-layer neural networks
				\item Calculating model errors, gradient descent, updating weights, and backpropagation
				\item Fitting the model and making predictions
			\end{itemize}

	\section{Revised Proposal/Plan}
		\textbf{Winter Break}
		\begin{itemize}
			\item Create preliminary model
			\item Train neural network using a subset of the data
			\item Compare different algorithms to find the best fit
			\item Optimise model and performing testing
		\end{itemize}
		\textbf{Semester 2 - Weeks 1-4}
		\begin{itemize}
			\item Finalise model and compile results
			\item Analyse findings and compare to current research
			\item Start a draft of final thesis
		\end{itemize}
		\textbf{Semester 2 - Weeks 5-8}
		\begin{itemize}
			\item Finish writing core component of thesis
			\item Summarise findings and conclude
			\item Perform further testing if time permits
		\end{itemize}
		\textbf{Semester 2 - Weeks 9-12}
		\begin{itemize}
			\item Prepare for final presentation
		\end{itemize}

	\begin{thebibliography}{99}
		\bibitem{Kong18} Kong, W. (2019). Short-Term Residential Load Forecasting Based on Resident Behaviour Learning - IEEE Journals \& Magazine. [online] Ieeexplore.ieee.org. Available at: \url{https://ieeexplore.ieee.org/document/7887751/}.

		\bibitem{Tian} Tian, C., Ma, J., Zhang, C. and Zhan, P. (2018). A Deep Neural Network Model for Short-Term Load Forecast Based on Long Short-Term Memory Network and Convolutional Neural Network. Energies, 11(12), p.3493.

		\bibitem{Warrior} Warrior, K., (2018). Short-Term Electrical Load Forecasting using Predictive Machine Learning Models

		\bibitem{Kong19} Kong, W. (2019). A Hierarchical Hidden Markov Model Framework for Home Appliance Modeling - IEEE Journals \& Magazine. [online] Ieeexplore.ieee.org. Available at: \url{https://ieeexplore.ieee.org/document/7738573/}.

		\bibitem{Bouktif} Bouktif, S., Fiaz, Ouni and Serhani (2019). Optimal Deep Learning LSTM Model for Electric Load Forecasting using Feature Selection and Genetic Algorithm: Comparison with Machine Learning Approaches †. [online] Ideas.repec.org. Available at: \url{https://ideas.repec.org/a/gam/jeners/v11y2018i7p1636-d153976.html}.

		\bibitem{KongL} Kong, L. (2017). Non-Intrusive Load Monitoring and Identification Based on Maximum Likelihood Method. Available at: \url{https://www.researchgate.net/publication/331898198_Non-intrusive_Load_Monitoring_Based_on_Convolutional_Neural_Network_Mixed_Residual_Unit}.



	
	\end{thebibliography}
\end{document}





























